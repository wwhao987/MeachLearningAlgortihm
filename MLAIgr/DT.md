### 训练内容

- tree structure
- threshold of classification
- The condition of stop 

1. 收集和准备数据：这一步骤主要涉及数据收集和准备，没有具体的公式。
2. 选择划分特征：常用的划分特征选择指标是信息增益（Information Gain）或基尼不纯度（Gini Impurity）。具体计算公式如下：
   - 信息增益： Information Gain(S, A) = Entropy(S) - ∑(|Sv| / |S|) * Entropy(Sv) 其中，S表示当前数据集，A表示划分特征，Sv表示根据划分特征A的取值v而得到的子集，Entropy(S)表示当前数据集的熵，Entropy(Sv)表示子集Sv的熵。
   - 基尼不纯度： Gini Impurity(S) = 1 - ∑(|Sv| / |S|)^2 其中，S表示当前数据集，Sv表示根据划分特征A的取值v而得到的子集。
3. 构建决策树：根据选择的划分特征，递归地构建决策树。没有具体的公式。
4. 划分数据集：根据选择的划分特征，将数据集划分为不同的子集。没有具体的公式。
5. 递归构建子树：递归地应用步骤2-4来构建子树。没有具体的公式。
6. 停止条件：根据具体的停止条件来决定是否停止划分。没有具体的公式。
7. 树的修剪：决策树修剪的目标是降低模型的过拟合程度，常用的方法是代价复杂度剪枝（Cost Complexity Pruning）。具体的公式如下：
   - 代价复杂度（Cost Complexity）： Cost(T) = Error(T) + α * Leaf(T) 其中，T表示决策树，Error(T)表示决策树的错误率，Leaf(T)表示决策树的叶节点数，α是一个调节参数。
   - 代价复杂度剪枝： Cost(T') <= Cost(T) + Δ(T, T') 其中，T'表示剪枝后的决策树，Δ(T, T')表示剪枝前后的损失。
8. 决策树的评估：评估决策树的性能和准确度，可以使用交叉验证、测试数据集等方法进行评估。没有具体的公式。
9. 使用决策树进行预测：根据构建好的决策树，进行新样本的预测。沿着树的分支根据条件判断，最终到达叶节点得到预测结果。没有具体的公式